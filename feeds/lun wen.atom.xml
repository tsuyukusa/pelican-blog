<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>電脳手帳 - 論文</title><link href="http://blog.calcurio.com/" rel="alternate"></link><link href="http://blog.calcurio.com/feeds/lun%20wen.atom.xml" rel="self"></link><id>http://blog.calcurio.com/</id><updated>2020-05-24T10:42:49+09:00</updated><subtitle>計算機環境やコーディング，機械学習まわりの話題を書き留めます</subtitle><entry><title>PALM_Machine_Learning_Explanations_for_Iterative_Debugging</title><link href="http://blog.calcurio.com/PALM_Machine_Learning_Explanations_for_Iterative_Debugging.html" rel="alternate"></link><published>2018-10-07T21:13:13+09:00</published><updated>2020-05-24T10:42:06+09:00</updated><author><name>M. Tsuyuki</name></author><id>tag:blog.calcurio.com,2018-10-07:/PALM_Machine_Learning_Explanations_for_Iterative_Debugging.html</id><summary type="html">&lt;p&gt;表題の論文を読んだときの私的メモ。まとまってないし，他人に読ませるために書いてない。&lt;/p&gt;
</summary><content type="html">&lt;p&gt;表題の論文を読んだときの私的メモ。まとまってないし，他人に読ませるために書いてない。&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;h1&gt;Abstract&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;DNNの誤分類の理由を知るのは難しい&lt;ul&gt;
&lt;li&gt;推論に影響を与えた学習データを絞るのが良い&lt;/li&gt;
&lt;li&gt;すべての学習データが多少は影響を与えているので分離が難しい&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;本論文で提案するPartition Aware Local Model (PALM)は，2つの代理モデルを使って特定する&lt;ul&gt;
&lt;li&gt;学習データを複数領域に分割するメタモデル&lt;ul&gt;
&lt;li&gt;メタモデルは決定木であり，人間が解釈可能なもの&lt;/li&gt;
&lt;li&gt;注目する推論結果に強い影響を与えた学習データの特定に利用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;領域内のパターンを近似するサブモデルたち&lt;ul&gt;
&lt;li&gt;サブモデルはどんなに複雑なモデルでも良い&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;PALMは従来法(nearest neighbor queries)よりも30倍高速であり，インタラクティブなデバッグを可能とする&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;1. Introduction&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;MLモデルの精度向上を助ける開発ツールが求められている&lt;ul&gt;
&lt;li&gt;モデルのデバッグと説明により，誤推論の原因を知る&lt;/li&gt;
&lt;li&gt;精度向上のための施策立案の補助&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;流行のDNNではブラックボックスモデルとして扱うしか無い&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;デバッグが非効率だった&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;データベース分野で機械学習のスケール性の研究が行われており，スケールするデバッグツールのための機は熟した&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方法の一つは，デバッグ対象のモデルを単純な代理モデルで近似すること&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;誤推論したテストデータの近傍の学習データを取り出し，線形モデルでフィットできる(Lime等)&lt;ul&gt;
&lt;li&gt;推論の根拠なった特徴量を大雑把に掴むことはできる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;推論に影響を与えた学習データ(のサブセット)も特定し，下記の3つに答えを与えたい&lt;ul&gt;
&lt;li&gt;推論に影響を与えた学習データはどれか&lt;/li&gt;
&lt;li&gt;学習データとテストデータの違いは？&lt;/li&gt;
&lt;li&gt;系統誤差と偶然誤差のどちらによる誤推論か&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;これにより，学習データをクレンジングするのか，学習データを増やすのか，高級なモデルを使うのか，判断を与えることができる&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;上記の目的から，学習データの分割によってモデルを説明づける方式を提案する&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;推論結果に与えた影響の大小で分割する&lt;/li&gt;
&lt;li&gt;ラベルか特徴量を変えれば，推論結果を変えられることを意味する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;データリネージュと説明文の関係に似ている&lt;ul&gt;
&lt;li&gt;ブラックボックスモデルは，データベースへのクエリ(複雑なSQLを人間にわかりやすくしようとする問題か？)&lt;/li&gt;
&lt;li&gt;機械学習では，全学習データが推論に影響を与えるため，難しい&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;高次元の特徴量空間の非線形モデル（NNなど）は複雑な分類境界を持つため，単にテストデータに最近傍の学習データを取り出すだけでは，間違いが多い&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;モデルの境界に従って学習データのサブセットを取り出す必要がある&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;本論文で提案するPartition Aware Local Model (PALM)は，2つの代理モデルを使って特定する&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;学習データを複数領域に分割するメタモデル&lt;ul&gt;
&lt;li&gt;メタモデルは決定木であり，人間が解釈可能なもの&lt;/li&gt;
&lt;li&gt;注目する推論結果に強い影響を与えた学習データの特定に利用&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;領域内のパターンを近似するサブモデルたち&lt;ul&gt;
&lt;li&gt;サブモデルはどんなに複雑なモデルでも良い&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;サブモデルは正確に，メタモデルによる分割ルールは解釈可能&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;2. Framework and API&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;解釈可能な特徴量と，解釈不可能な特徴寮の混ざったデータ・セットにおいてPALMはEM-likeな合うr後リズムで，解釈可能な特徴量についてモデルの振る舞いを再現するように分割するルールを学習する&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;2.1 Notation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;特徴量x_i とラベルy_i のタプルからなるデータセットD&lt;/li&gt;
&lt;li&gt;train(D)はmodel()を返す学習関数&lt;/li&gt;
&lt;li&gt;テストデータ x_new&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;テストデータの推論結果 y_new\hat = model(x_new)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;保険金詐欺の検知タスクであれば&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;D(make, amount, at.fault, descr, fraud?)&lt;ul&gt;
&lt;li&gt;make: 車の型番 [categorical]&lt;/li&gt;
&lt;li&gt;amount: 保険金の請求額 [double]&lt;/li&gt;
&lt;li&gt;at.fault: 請求者の過失の有無 [boolean]&lt;/li&gt;
&lt;li&gt;descr: 請求の内容 [string]&lt;/li&gt;
&lt;li&gt;fraud: 詐欺か，詐欺で無いか&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;シンプルな決定木などであれば，モデルを解釈して推論結果を確認できる&lt;ul&gt;
&lt;li&gt;詐欺の場合，過失がないと主張していることが多い，など&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;descrは詐欺の分類に重要だが，NLPのword-embeddingsやstop wordの削除，bi-gramなどで特徴量をつくるため，解釈が困難な問題がある&lt;ul&gt;
&lt;li&gt;その上，シンプルなモデルでは精度を出せない&lt;/li&gt;
&lt;li&gt;テストデータの近傍点をとっても，推論結果が異なる可能性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;2.2 Debugging with PALM&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;下記のようなイメージで，データを分割するコードを生成（学習）する&lt;ul&gt;
&lt;li&gt;metamodelは人間が理解できる簡単なモデル&lt;ul&gt;
&lt;li&gt;解釈可能な特徴量についてのみ分割する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;submodelは人間が理解できない複雑なモデルで，元モデルの振る舞いを正確に再現するために利用&lt;ul&gt;
&lt;li&gt;submodelの学習はmetamodelで分割されたそれぞれの領域で実行&lt;/li&gt;
&lt;li&gt;submodelの学習には解釈困難な特徴量を含む&lt;/li&gt;
&lt;li&gt;問題があったときは，元モデル全体ではなく，submodelを調査すれば良いことになる&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;メタモデルの分割数の決定が一つのポイント&lt;ul&gt;
&lt;li&gt;少なすぎると，デバッグに役立たない&lt;ul&gt;
&lt;li&gt;極限は全データを一つの領域に分割&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;多すぎると，元のモデルの振る舞いを再現できない&lt;ul&gt;
&lt;li&gt;極限は全学習データについて単純に決定木を学習する場合&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;metamodel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;amount&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;#one model for larger claims&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;submodel_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;elif&lt;/span&gt;  &lt;span class="n"&gt;a_fault&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;#one model for small at fault claims&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;submodel_2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;#a default model&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;submodel_3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;メタモデルの学習は，元のモデルの学習時に行えば良い&lt;/li&gt;
&lt;li&gt;推論時に元のモデルとメタモデルによる推論を実行すれば良い&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;2.3 API and System&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;入力は&lt;ul&gt;
&lt;li&gt;データセット&lt;/li&gt;
&lt;li&gt;解釈可能な特徴量名のリスト&lt;/li&gt;
&lt;li&gt;解釈したいモデルのパラメータ(学習前)&lt;/li&gt;
&lt;li&gt;サブモデルの数k&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;出力は&lt;ul&gt;
&lt;li&gt;解釈対象の学習済みモデル&lt;/li&gt;
&lt;li&gt;k個のサブモデル (アルゴリズムは解釈対象のモデルと同一？)&lt;/li&gt;
&lt;li&gt;メタモデル(決定木)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;3. Algorithm Description&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;K-mearnsと同じように，k個のランダムなサブモデルを初期化&lt;/li&gt;
&lt;li&gt;最も高精度に推論できるサブモデルにデータ点を帰属させる&lt;/li&gt;
&lt;li&gt;gradient descentでモデルのパラメータを更新する&lt;/li&gt;
&lt;li&gt;Expectation-Maximization algorithmで，Maximization stepがgradientを計算する亜種である&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;3.1 Technical Details&lt;/h2&gt;
&lt;h3&gt;Fitting Step&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Modelの出力を教師としてmetamodelを作る&lt;ul&gt;
&lt;li&gt;任意の確率密度の近似は困難なので，kこのサブモデルで近似するf(y\hat |x,\theta)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Distillation  Step&lt;/h3&gt;</content><category term="論文"></category><category term="機械学習"></category></entry><entry><title>A Survey of Methods For Explaining Bloack Box Modelsを読んだ</title><link href="http://blog.calcurio.com/A_Survey_of_Methods_For_Explaining_Bloack_Box_Models.html" rel="alternate"></link><published>2018-10-07T13:40:52+09:00</published><updated>2020-05-24T10:42:49+09:00</updated><author><name>M. Tsuyuki</name></author><id>tag:blog.calcurio.com,2018-10-07:/A_Survey_of_Methods_For_Explaining_Bloack_Box_Models.html</id><summary type="html">&lt;p&gt;機械学習モデルの解釈可能性に関する，サーベイ論文として重要な
[1802.01933] A Survey Of Methods For Explaining Black Box Models - https://arxiv.org/abs/1802.01933 を読んだのでメモ。&lt;/p&gt;
</summary><content type="html">&lt;p&gt;機械学習モデルの解釈可能性に関する，サーベイ論文として重要な
[1802.01933] A Survey Of Methods For Explaining Black Box Models - https://arxiv.org/abs/1802.01933 を読んだのでメモ。&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;h1&gt;Abstract&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;多くの意思決定システムはブラックボックスであり，ユーザにとって内部ロジックは不明&lt;/li&gt;
&lt;li&gt;解釈可能性を向上させる研究は，下記の3つに応じて多数ある&lt;ul&gt;
&lt;li&gt;問題の定義&lt;/li&gt;
&lt;li&gt;ブラックボックスのアルゴリズム&lt;/li&gt;
&lt;li&gt;望まれる「説明」の定義&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;本サーベイ論文では上記の3つに応じてた研究をまとめる&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;1 Indtroduction&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;分類・回帰にかかわらず，学習データを作った人間のバイアスがモデルに反映され，誤った推論・意思決定をする可能性がある&lt;/li&gt;
&lt;li&gt;GDPR(EU一般データ保護規則)では，自動化された意思決定システムにおいて，ロジックについて説明を受ける権利が規定されている [1]&lt;ul&gt;
&lt;li&gt;与信審査，雇用，保険契約などにおいて，偏見や差別のない構成で透明性のある処理を実現するため&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;自動運転や医療の個人最適化など，信頼性が求められるタスクでも，データの偏りによる誤推論のリスクがある&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;故にブラックボックスモデルを解釈する研究が求められている&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;モデルを説明するものと，推論結果を説明するもの&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;残りの節では下記を議論する&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3: 解釈性とは何か&lt;/li&gt;
&lt;li&gt;4: カテゴライズするための問題定義&lt;/li&gt;
&lt;li&gt;5-9: 先行研究の紹介&lt;/li&gt;
&lt;li&gt;10: まとめ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;[1] &lt;a href="https://blog.cloudera.co.jp/eu%E4%B8%80%E8%88%AC%E3%83%87%E3%83%BC%E3%82%BF%E4%BF%9D%E8%AD%B7%E8%A6%8F%E5%89%87-gdpr-%E3%81%A8%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%82%B9-39d03f775f59"&gt;EU一般データ保護規則 (GDPR) とデータサイエンス – Cloudera Japan Official Blog &lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;2 Needs for Interpretable Models&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;実際に解釈可能性が必要になるケースを紹介する&lt;ul&gt;
&lt;li&gt;社会的に問題&lt;ul&gt;
&lt;li&gt;採用面接の対象者選定: 女性や人種的マイノリティが排除&lt;/li&gt;
&lt;li&gt;犯罪の常習性: 黒人が倍に&lt;/li&gt;
&lt;li&gt;Webテキストからの感情分析: 黒人系の名前が出現すると否定的な感情に分類するモデル&lt;/li&gt;
&lt;li&gt;Amazon.comの無料お急ぎ便クーポン: 人種的マイノリティが除外&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;学習データの偏り&lt;ul&gt;
&lt;li&gt;友軍と敵軍戦車の画像分類: 敵軍は曇り，友軍は晴れの日の写真が多いいため，低汎化性能&lt;/li&gt;
&lt;li&gt;狼とハスキー犬の画像分類: 背景の雪を根拠に狼と分類していた&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;攻撃&lt;ul&gt;
&lt;li&gt;ホワイトノイズに見える画像を，DNNにトマトと分類させた研究&lt;/li&gt;
&lt;li&gt;人間にはわからないが，DNNのテキスト分類結果を騙す摂動を加えた研究&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;これらは学習データの偏りを反映したもの&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;3 Interpretable, Explainable and Comprehensible Models&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;解釈可能なモデルとは何か示す&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;3.1 Dimensions of Interpretability&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;解釈性を分類する指標には，次の3つがある&lt;ul&gt;
&lt;li&gt;大域的・局所的 (Global/Local) 解釈性&lt;ul&gt;
&lt;li&gt;大域的: すべての入力データについてモデルの振る舞いを解釈&lt;/li&gt;
&lt;li&gt;局所的: 特定の入力データ(推論結果)についてモデルの振る舞いを解釈&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;計算時間&lt;ul&gt;
&lt;li&gt;ユーザが解釈を閲覧して，直ちに決断する(災害対応など)か，時間的余裕があるか&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;ユーザの専門性&lt;ul&gt;
&lt;li&gt;モデルが解くタスクに，ユーザが知識を持っているか&lt;/li&gt;
&lt;li&gt;Domain expertsならば複雑・詳細なモデルの説明を好む。素人ならばシンプルなモデルの説明が良い&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;3.2 Desiderata of an Interpretable Model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;解釈可能なモデルに対する3つの要求&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;解釈性(Interpretability)&lt;ul&gt;
&lt;li&gt;モデルや推論結果を人間が理解できるか&lt;/li&gt;
&lt;li&gt;指標はモデルサイズ（モデルの複雑性）&lt;/li&gt;
&lt;li&gt;理解可能性(Comprehensibility)とも&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;精度&lt;ul&gt;
&lt;li&gt;新しいデータを解釈可能なモデルが正しく分類できるか&lt;/li&gt;
&lt;li&gt;指標は，テストデータに対するaccuracyやF1スコアなど&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;忠実性&lt;ul&gt;
&lt;li&gt;説明対象のブラックボックスモデルの再現性。&lt;/li&gt;
&lt;li&gt;指標は，ブラックボックスモデルをオラクルとしたaccuracyやF1スコアで定義&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;倫理的観点からは，下記も大事な指標&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;公平性&lt;ul&gt;
&lt;li&gt;特定グループへの意識的・無意識的な差別&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;プライバシー&lt;ul&gt;
&lt;li&gt;モデルから個人情報の流出&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;人間に信用されるモデルとしてはusabilityも大事&lt;ul&gt;
&lt;li&gt;固定的なモデルよりも，クエリによって振る舞いの変わる柔軟場モデルのほうが，情報量が多いので人間は有用とみなす&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;3.3 Recognized Interpretable Models&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;決定木，decision rule，線形モデルは解釈可能とみなされている&lt;ul&gt;
&lt;li&gt;決定木はif-then形式で記述可能&lt;ul&gt;
&lt;li&gt;if condition1 ∧ condition2 ∧ condition3 then outcome.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;decision ruleは一般化された，classification ruleからなるもの&lt;ul&gt;
&lt;li&gt;決定木の形式にこだわらないif-then&lt;/li&gt;
&lt;li&gt;m-fo-n rule: n個のルールのうち，m個が満たされたら真&lt;/li&gt;
&lt;li&gt;list of rule:&lt;/li&gt;
&lt;li&gt;falling rule lists:&lt;/li&gt;
&lt;li&gt;decision sets:&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;3.4 Explanations and Interpretable Models Complexity&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;モデルサイズ&lt;ul&gt;
&lt;li&gt;決定木: ノード数&lt;/li&gt;
&lt;li&gt;線形モデル; 変数の数&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;3.5 Interpretable Data for Interpretable Models&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Table形式のデータであれば人間も解釈しやすい&lt;ul&gt;
&lt;li&gt;ベクトル形式のデータにそのまま対応しているので機械学習と親和性高い&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;その他には画像とテキスト&lt;/li&gt;
&lt;li&gt;時系列データなどの例は無い&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;4 Open The Black Box Problems&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;解釈可能なモデルを作るアプローチは2つ&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ブラックボックスモデルをリバースエンジニアリングする&lt;/li&gt;
&lt;li&gt;最初から解釈可能なモデルをデザインする&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;リバースエンジニアリングでは，ブラックボックスモデルの学習に使ったデータ・セットは利用不可能な事が多い&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;4つの問題を定義&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;black box model explanation problem&lt;ul&gt;
&lt;li&gt;大域的に解釈可能なモデルで近似する&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;black box outcome explanation problem&lt;ul&gt;
&lt;li&gt;推論結果とともにその理由を示す。局所的な解釈性させあれば良い&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;black box inspection problem.&lt;ul&gt;
&lt;li&gt;ブラックボックスモデルの感度分析&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;transparent box design problem&lt;ul&gt;
&lt;li&gt;ブラックボックスモデルとは無関係に，最初から解釈可能なモデルを作る&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;5 Problem And Explanator Based Classification&lt;/h1&gt;</content><category term="論文"></category><category term="機械学習"></category></entry></feed>